{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl the Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawling:\n",
    "    def __init__(self, root_link, crawl_level):\n",
    "        self.root = root_link\n",
    "        self.level = crawl_level\n",
    "        self.root_dir = ''\n",
    "    def run(self):\n",
    "        self.root_dir = self.creat_dir()\n",
    "        link_list = [self.root]\n",
    "        for level in range(0,self.level):  \n",
    "            level_links = []\n",
    "            level_para = ''\n",
    "            print(\"\\n Level \",str(level))\n",
    "            for link in tqdm(link_list):\n",
    "                link_list, para = self.crawl_link(link) \n",
    "                level_links += link_list\n",
    "                level_para += para\n",
    "            f = open(os.path.join(self.root_dir, \"level\"+str(level)+\".txt\"),\"w+\")\n",
    "            f.write(level_para)\n",
    "            f.close()\n",
    "            link_list = level_links\n",
    "            time.sleep(15)\n",
    "\n",
    "        filenames = []\n",
    "        for file in glob.glob(self.root_dir+\"/*.*\"):\n",
    "            filenames.append(file.split(\"/\")[1])\n",
    "        if 'levels.txt' in filenames:\n",
    "            filenames.remove('levels.txt')\n",
    "  \n",
    "        with open(os.path.join(self.root_dir, \"levels.txt\"), 'w') as outfile:\n",
    "            for fname in filenames:\n",
    "                with open(os.path.join(self.root_dir, fname)) as infile:\n",
    "                    for line in infile:\n",
    "                        outfile.write(line) \n",
    "    def creat_dir(self):\n",
    "        temp = ''\n",
    "        for char in self.root:\n",
    "            if char in [':', '/', '.']:\n",
    "                temp += '_'\n",
    "            else: \n",
    "                temp += char\n",
    "        try:\n",
    "            os.mkdir(temp)\n",
    "        except OSError:  \n",
    "            print(\"Directory Exists!\")\n",
    "        return temp\n",
    "    def remove_tags(self, p):\n",
    "        p = str(p)\n",
    "        temp = ''\n",
    "        tagflag = 0\n",
    "        for char in p:\n",
    "            if char == \"<\":\n",
    "                tagflag = 1\n",
    "            if char == \">\":\n",
    "                tagflag = 0\n",
    "                continue\n",
    "            if tagflag == 0:\n",
    "                temp += char \n",
    "        return temp\n",
    "    def remove_bm(self, p):\n",
    "        p = str(p)\n",
    "        temp = ''\n",
    "        tagflag = 0\n",
    "        for char in p:\n",
    "            if char == \"[\":\n",
    "                tagflag = 1\n",
    "            if char == \"]\":\n",
    "                tagflag = 0\n",
    "                continue\n",
    "            if tagflag == 0: \n",
    "                temp += char \n",
    "        return temp\n",
    "    def remove_brak(self, p):\n",
    "        p = str(p)\n",
    "        temp = ''\n",
    "        tagflag = 0\n",
    "        for char in p:\n",
    "            if char == \"(\":\n",
    "                tagflag = 1\n",
    "            if char == \")\":\n",
    "                tagflag = 0\n",
    "                continue\n",
    "            if tagflag == 0: \n",
    "                temp += char \n",
    "        return temp\n",
    "    def remove_num(self, p):\n",
    "        p = str(p)\n",
    "        temp = '' \n",
    "        for char in p:\n",
    "            if char in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "                temp += ''\n",
    "            else:\n",
    "                temp += char\n",
    "        return temp\n",
    "    def edit_links(self, link_list):\n",
    "        for i in range(len(link_list)-1 , -1 , -1 ):\n",
    "            if link_list[i] is None:\n",
    "                link_list.remove(link_list[i])\n",
    "\n",
    "        for i in range(len(link_list)-1 , -1 , -1 ):\n",
    "            if 'wiki' not in str(link_list[i]):\n",
    "                link_list.remove(link_list[i])\n",
    "        for i in range(0, len(link_list)):\n",
    "            if str(link_list[i][:6]) == '/wiki/':\n",
    "                link_list[i] = 'https://en.wikipedia.org' + link_list[i]\n",
    "        #filter wiki tools\n",
    "        for i in range(len(link_list)-1 , -1 , -1 ):\n",
    "            if str(link_list[i][:2]) == '//':\n",
    "                link_list.remove(link_list[i])\n",
    "        #english pages\n",
    "        for i in range(len(link_list)-1 , -1 , -1 ):\n",
    "            if 'https://en.' not in str(link_list[i]):\n",
    "                link_list.remove(link_list[i])\n",
    "        #Filter Images Links\n",
    "        for i in range(len(link_list)-1 , -1 , -1 ):\n",
    "            if '.jpg' in str(link_list[i]) or '.png' in str(link_list[i]) or '.jpeg' in str(link_list[i]):\n",
    "                link_list.remove(link_list[i])\n",
    "        return link_list\n",
    "    def crawl_link (self, link):\n",
    "        links = []\n",
    "        paragraph = ''\n",
    "        root = requests.get(link)\n",
    "        if root.status_code == 200:\n",
    "            #Code\n",
    "            #<div id=\"bodyContent\" class=\"mw-body-content\">\n",
    "            soup = BeautifulSoup(root.content)\n",
    "            samples = soup.find_all(\"div\", \"mw-body-content\")\n",
    "            #get all links in the page \n",
    "            for l in soup.find_all('a'):\n",
    "                links.append(l.get('href'))\n",
    "            links = self.edit_links(links)\n",
    "            #get all paragraphs\n",
    "            parags = soup.find_all('p')\n",
    "            #extract paragraphs without html tags\n",
    "            for i in range(0,len(parags)):\n",
    "                paragraph += self.remove_brak(self.remove_num(self.remove_bm(self.remove_tags(parags[i]))))\n",
    "        else:\n",
    "            print(\"Page Not Found!\")\n",
    "        return links, paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory Exists!\n",
      "\n",
      " Level  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgd-pc/.local/lib/python3.5/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/mgd-pc/anaconda3/lib/python3.5/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n",
      "  0%|          | 0/637 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Level  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637/637 [18:59<00:00,  1.79s/it]\n"
     ]
    }
   ],
   "source": [
    "crawl = crawling(\"https://en.wikipedia.org/wiki/Dentistry\",2)\n",
    "crawl.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessU(text):\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', ' <PERIOD> ')\n",
    "    text = text.replace(',', ' <COMMA> ')\n",
    "    text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
    "    text = text.replace(';', ' <SEMICOLON> ')\n",
    "    text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    text = text.replace('(', ' <LEFT_PAREN> ')\n",
    "    text = text.replace(')', ' <RIGHT_PAREN> ')\n",
    "    text = text.replace('--', ' <HYPHENS> ')\n",
    "    text = text.replace('?', ' <QUESTION_MARK> ')\n",
    "    # text = text.replace('\\n', ' <NEW_LINE> ')\n",
    "    text = text.replace(':', ' <COLON> ')\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove all words with  5 or fewer occurences\n",
    "    word_counts = Counter(words)\n",
    "    trimmed_words = [word for word in words if word_counts[word] > 5]\n",
    "\n",
    "    return trimmed_words\n",
    "\n",
    "def get_batchesU(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: A list where each item is a tuple of (batch of input, batch of target).\n",
    "    \"\"\"\n",
    "    n_batches = int(len(int_text) / (batch_size * seq_length))\n",
    "\n",
    "    # Drop the last few characters to make only full batches\n",
    "    xdata = np.array(int_text[: n_batches * batch_size * seq_length])\n",
    "    ydata = np.array(int_text[1: n_batches * batch_size * seq_length + 1])\n",
    "\n",
    "    x_batches = np.split(xdata.reshape(batch_size, -1), n_batches, 1)\n",
    "    y_batches = np.split(ydata.reshape(batch_size, -1), n_batches, 1)\n",
    "\n",
    "    return list(zip(x_batches, y_batches))\n",
    "\n",
    "\n",
    "def create_lookup_tablesU(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    n_batches = len(words)//batch_size\n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dentistry', '<COMMA>', 'also', 'known', 'as', 'dental', 'and', 'oral', 'medicine', '<COMMA>', 'is', 'a', 'branch', 'of', 'medicine', 'that', 'consists', 'of', 'the', 'study', '<COMMA>', 'diagnosis', '<COMMA>', 'prevention', '<COMMA>', 'and', 'treatment', 'of', 'diseases', '<COMMA>']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(crawl.root_dir, \"levels.txt\")) as f:\n",
    "    text = f.read()\n",
    "words = preprocessU(text)\n",
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 1370744\n",
      "Unique words: 12885\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tablesU(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1e-5\n",
    "word_counts = Counter(int_words)\n",
    "total_count = len(int_words)\n",
    "freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding = 300 # Number of embedding features \n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mgd-pc/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_vocab))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_vocab)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-21-27daed5c5040>:14: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch_size = 1000\n",
    "window_size = 5\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x, labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 1000 == 0:\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_vocab[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_words = 2500\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.models import ColumnDataSource, Range1d, LabelSet, Label\n",
    "\n",
    "%time\n",
    "viz_words = 1000\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "output_notebook()\n",
    "desc = []\n",
    "for idx in range(viz_words):\n",
    "    desc.append(int_to_vocab[idx])\n",
    "    \n",
    "list_x = list(embed_tsne[0:viz_words, 0])\n",
    "list_y = list(embed_tsne[0:viz_words, 1])\n",
    "\n",
    "source = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc)) \n",
    "hover = HoverTool(tooltips=[\n",
    "    ('desc', '@desc'),\n",
    "])\n",
    "mapper = LinearColorMapper(palette=plasma(256), low=min(list_y), high=max(list_y))\n",
    "\n",
    "p = figure(plot_width=1000, plot_height=1000, tools=[hover,\"pan,wheel_zoom,box_zoom\"], \n",
    "           title=\"Word2Vec\")\n",
    "p.circle('x', 'y', size=10, source=source)\n",
    "\n",
    "labels = LabelSet(x='x', y='y', text='desc', level='glyph',\n",
    "              x_offset=5, y_offset=5, source=source, render_mode='canvas')\n",
    "\n",
    "p.add_layout(labels)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
