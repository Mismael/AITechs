{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imported Libraries\n",
    "\"\"\"\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.regularizers import l1 , l2 , l1_l2\n",
    "from keras.optimizers import Adam , SGD\n",
    "from keras import regularizers\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "import keras\n",
    "import glob\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Variables\n",
    "\"\"\"\n",
    "input_shape = 32\n",
    "no_classes = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model Layers\n",
    "\"\"\"\n",
    "classifier = Sequential()\n",
    "layer_00 = Conv2D(6, (5, 5),input_shape = (input_shape,input_shape, 3), activation = 'relu')\n",
    "classifier.add(layer_00) \n",
    "classifier.add(BatchNormalization(axis=-1))\n",
    "\n",
    "layer_00_w = layer_00.get_weights()[0][:,:,0,:]\n",
    "for i in range(1,26):\n",
    "    plt.subplot(5,5,i)\n",
    "    plt.imshow(layer_00_w[:,:,i],interpolation=\"nearest\",cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "layer_01 = Conv2D(32, (3, 3), activation = 'relu')\n",
    "classifier.add(layer_01)\n",
    "classifier.add(BatchNormalization(axis=-1))\n",
    "\n",
    "layer_01_w = layer_01.get_weights()[0][:,:,0,:]\n",
    "for i in range(1,26):\n",
    "    plt.subplot(5,5,i)\n",
    "    plt.imshow(layer_01_w[:,:,i],interpolation=\"nearest\",cmap=\"gray\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "classifier.add(Conv2D(64,(3, 3), activation = 'relu'))\n",
    "classifier.add(BatchNormalization(axis=-1))\n",
    "classifier.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "classifier.add(BatchNormalization(axis=-1))\n",
    "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "cnn_outdim = classifier.layers[len(classifier.layers) - 1].output.get_shape()[1]\n",
    "print(cnn_outdim)\n",
    "\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(output_dim = 120, activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(output_dim = 84, activation = 'relu'))\n",
    "classifier.add(BatchNormalization())\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(output_dim = no_classes, activation = 'softmax'))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "classifier = Sequential()\n",
    "\n",
    "classifier.add(Conv2D(filters=8, kernel_size=2, input_shape=(224, 224, 3), activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=2, data_format='channels_last'))\n",
    "classifier.add(Conv2D(filters=16, kernel_size=2, activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=2, data_format='channels_last'))\n",
    "classifier.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "classifier.add(MaxPooling2D(pool_size=2, data_format='channels_last'))\n",
    "classifier.add(GlobalAveragePooling2D())\n",
    "classifier.add(Dense(no_classes, activation='relu'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compile variables and Data Generation\n",
    "\"\"\"\n",
    "\n",
    "#sgd = SGD (lr = 0.001  , nesterov = True)\n",
    "adam = Adam (lr=0.001)\n",
    "#classifier.compile(loss = 'categorical_crossentropy',  metrics = [\"accuracy\"], optimizer = Adam(lr=1e-3))\n",
    "classifier.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = adam)\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=8, width_shift_range=0.1, shear_range=0.3,\n",
    "                         height_shift_range=0.1, zoom_range=0.08)\n",
    "test_datagen = ImageDataGenerator( )\n",
    "\n",
    "#CLOUD PATH\n",
    "#dir_train = '/job2/train'\n",
    "#dir_test =  '/job2/test'\n",
    "\n",
    "#Desktop PATH\n",
    "dir_train = 'train'\n",
    "dir_test =  'test'\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(dir_train,\n",
    "                                                 target_size = (input_shape,input_shape),\n",
    "                                                 batch_size = 64,\n",
    "                                                 class_mode = 'categorical')\n",
    "test_set = test_datagen.flow_from_directory(dir_test,\n",
    "                                            target_size = (input_shape,input_shape),\n",
    "                                            batch_size = 64,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train Model\n",
    "\"\"\"\n",
    "\n",
    "class test(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.letters_list = ['beh' , 'teh' , 'seh' , 'gem' , 'ha' , 'kha' ,\n",
    "                             'sen' , 'shen' , 'sad' , 'dad' , 'tah' , 'zah' ,\n",
    "                             'ayen' , 'ghen', 'feh' , 'kaaf' , 'kaf' , 'lam',\n",
    "                             'mem' , 'non' , 'heh' , 'yeh' , 'hamza']\n",
    "        self.rightl = [15,17,6,21,1,15,6,21,10,10,21,3,21,17,\n",
    "                       18,4,18,18,4,12,21,17,18,19,8,2,17,17,21,19,15]\n",
    "        self.outputlist = []\n",
    "        self.predicted = []\n",
    "        self.cnt = 0\n",
    "        for file in glob.glob(\"*.png\"):\n",
    "            self.img = cv2.imread(file)\n",
    "            self.imresize = cv2.resize(self.img, (input_shape , input_shape))\n",
    "            self.imresize = cv2.bitwise_not(self.imresize)\n",
    "            self.imresize = cv2.cvtColor(self.imresize , cv2.COLOR_BGR2GRAY)\n",
    "            ret,self.thresh1 = cv2.threshold(self.imresize,127,255,cv2.THRESH_BINARY)\n",
    "            self.thresh1 = cv2.cvtColor(self.thresh1, cv2.COLOR_GRAY2BGR)\n",
    "            self.imlist = np.array([self.thresh1])\n",
    "            self.predhis = (classifier.predict_classes(self.imlist))\n",
    "            self.outputlist.append(self.predhis[0])\n",
    "        for i in range (0 , len(self.rightl)-1):\n",
    "            if self.outputlist[i] == self.rightl[i]:\n",
    "                self.cnt += 1\n",
    "                self.predicted.append(self.outputlist[i])\n",
    "\n",
    "        print(self.cnt)\n",
    "        print(\"\\n\")\n",
    "        print(self.predicted)\n",
    "        self.now =  datetime.datetime.now()\n",
    "        self.y = str(self.now.date().year)\n",
    "        self.m = str(self.now.date().month)\n",
    "        self.d = str(self.now.date().day)\n",
    "        self.h = str(self.now.hour)\n",
    "        self.mi = str(self.now.minute)\n",
    "        self.name = \"Dvx_\"+self.y+self.m+self.d+\"_\"+self.h+self.mi+\" - \"+str(self.cnt)\n",
    "        #CLOUD PATH\n",
    "        #self.dir_model = \"/output/\"+self.name+\".h5\"\n",
    "        #Desktop PATH\n",
    "        self.dir_model = self.name+\".h5\"\n",
    "        classifier.save(self.dir_model)\n",
    "\n",
    "predictions=test()\n",
    "history = classifier.fit_generator(training_set,\n",
    "                         samples_per_epoch = 17843,\n",
    "                         nb_epoch = 100,\n",
    "                         validation_data = test_set,\n",
    "                         nb_val_samples = 4686,\n",
    "                         callbacks=[predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save Model\n",
    "\"\"\"\n",
    "\n",
    "now =  datetime.datetime.now()\n",
    "y = str(now.date().year)\n",
    "m = str(now.date().month)\n",
    "d = str(now.date().day)\n",
    "h = str(now.hour)\n",
    "mi = str(now.minute)\n",
    "s = str(now.second)\n",
    "name = \"Dvx_\"+y+m+d+\"_\"+h+mi\n",
    "\n",
    "#CLOUD PATH\n",
    "#dir_model = \"/output/\"+name+\".h5\"\n",
    "#Desktop PATH\n",
    "dir_model = name+\".h5\"\n",
    "\n",
    "classifier.save(dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot Acc & Loss\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.subplot(212)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.show()\n",
    "plt.savefig(\"/output/\"+name+\".png\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# \"\"\"\n",
    "# Model Validation\n",
    "# \"\"\"\n",
    "def original_and_inverted (file, shape, obj_list, Model):\n",
    "    img = cv2.imread(file)\n",
    "    imresize = cv2.resize(img, (shape , shape))\n",
    "    \n",
    "    ret,thresh0 = cv2.threshold(imresize.copy(),127,255,cv2.THRESH_BINARY)\n",
    "    imlist = np.array([thresh0])\n",
    "    classes0 =Model.predict_classes(imlist)\n",
    "\n",
    "    img_inv = cv2.bitwise_not(imresize.copy())\n",
    "    ret,thresh1 = cv2.threshold(img_inv.copy(),127,255,cv2.THRESH_BINARY)\n",
    "    imlist = np.array([thresh1])\n",
    "    classes1 =Model.predict_classes(imlist)\n",
    "\n",
    "    print(\"Original Image: \"+obj_list[classes0[0]]+\"  Inverted Image: \"+obj_list[classes1[0]] )\n",
    "\n",
    "\n",
    "letters_list = ['beh' , 'teh' , 'seh' , 'gem' , 'ha' , 'kha' ,\n",
    "                  'sen' , 'shen' , 'sad' , 'dad' , 'tah' , 'zah' ,\n",
    "                  'ayen' , 'ghen', 'feh' , 'kaaf' , 'kaf' , 'lam',\n",
    "                  'mem' , 'non' , 'heh' , 'yeh' , 'hamza']\n",
    "\n",
    "classifier = load_model(\"Dvx_20171221_2058 - 17.h5\")\n",
    "classifier.compile(loss = 'categorical_crossentropy', metrics = ['accuracy'], optimizer = 'adam')\n",
    " \n",
    " \n",
    "original_and_inverted(\"30.png\", input_shape, letters_list, classifier)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
